 app.py
import os
import time
import streamlit as st
from pathlib import Path
from rag_utils import DocumentStore, build_chunks_for_file  # caching store + helpers
from ollama_utils import OllamaClient

---- config ----
st.set_page_config(layout="wide", page_title="PDF RAG Chat (Streamlit)", initial_sidebar_state="expanded")
 Default example file (the file you uploaded earlier)
DEFAULT_DOC_PATH = r"D:/rag_project/uploads/cf181d394bcc4c599fe4b4368df65a1e.pdf"

# Instantiate utilities
DOC_STORE = DocumentStore(cache_dir=".emb_cache")   # on-disk cache for embeddings
OLLAMA = OllamaClient()  # reads OLLAMA_HOST / OLLAMA_MODEL env vars

# ---- sidebar: upload + library ----
with st.sidebar:
    st.title("Documents")
    uploaded = st.file_uploader("Upload PDF(s)", type=["pdf"], accept_multiple_files=True)
    st.write("Or use example")
    if st.button("Use example document"):
        uploaded = [open(DEFAULT_DOC_PATH, "rb")]

    # Load uploaded files into DocumentStore
    if uploaded:
        for f in uploaded:
            # if Streamlit provided a SpooledTemporaryFile, save to uploads
            fname = getattr(f, "name", None) or f"uploaded_{int(time.time())}.pdf"
            target = Path("uploads") / fname
            target.parent.mkdir(exist_ok=True)
            # If f is a BytesIO (uploaded), write bytes, else if file path string just ignore
            if hasattr(f, "read"):
                with open(target, "wb") as out:
                    out.write(f.read())
            DOC_STORE.add_file(str(target))

    st.markdown("### Library")
    docs = DOC_STORE.list_docs()
    if not docs:
        st.info("No documents. Upload PDFs or click Use example.")
    doc_choice = st.selectbox("Select document", options=docs, format_func=lambda p: Path(p).name if p else "")
    st.write("Selected:", Path(doc_choice).name if doc_choice else "—")

    st.markdown("---")
    st.markdown("**Model settings**")
    model_name = st.text_input("Ollama model", value=os.getenv("OLLAMA_MODEL", "llama3"))
    st.checkbox("Stream responses", value=True, key="stream_enabled")
    if st.button("Refresh embeddings for selected doc"):
        if doc_choice:
            with st.spinner("Rebuilding embeddings..."):
                DOC_STORE.rebuild_embeddings(doc_choice)
            st.success("Rebuilt embeddings.")

# ---- main layout: left = doc summary, right = chat ----
left, right = st.columns([2, 5])

with left:
    st.header("Document")
    if doc_choice:
        meta = DOC_STORE.get_meta(doc_choice)
        st.markdown(f"**File:** {Path(doc_choice).name}")
        st.markdown(f"**Extracted chunks:** {meta['chunk_count']}")
        st.markdown(f"**Chunk size:** {meta.get('chunk_size', 1000)}")
        if st.button("Show sample chunks"):
            chunks = DOC_STORE.get_chunks(doc_choice)
            for i, c in enumerate(chunks[:5]):
                st.markdown(f"**Chunk {i}** — {len(c)} chars")
                st.write(c[:1000])
    else:
        st.info("Select or upload a document from the sidebar.")

    st.markdown("---")
    st.header("Question Generator")
    if doc_choice:
        q_mode = st.selectbox("Mode", ["mcq", "short", "long"])
        q_count = st.slider("Number of questions", 1, 10, 4)
        q_include_answers = st.checkbox("Include answers", value=True)
        if st.button("Generate Questions"):
            with st.spinner("Generating questions..."):
                # build context from top chunks
                chunks = DOC_STORE.get_chunks(doc_choice)
                context = "\n\n---\n\n".join(chunks[:8])
                prompt = f"You are a question generator. Create {q_count} {q_mode} questions from the context. Include answers: {q_include_answers}. Context:\n{context}\nReturn only JSON."
                try:
                    resp = OLLAMA.generate(prompt, timeout=60)
                except Exception as e:
                    st.error(f"Ollama error: {e}")
                    resp = None
                st.subheader("Raw Response")
                st.code(resp or "No response")

with right:
    st.header("Chat")
    if not doc_choice:
        st.info("Select a document to start chatting.")
    else:
        # chat UI state stored in session_state
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if "history" not in st.session_state:
            st.session_state.history = []

        # input area
        user_input = st.text_input("Ask a question about the document", key="user_input")
        col1, col2 = st.columns([1, 3])
        with col1:
            top_k = st.number_input("Top K chunks", value=6, min_value=1, max_value=20)
            mode = st.selectbox("Mode", ["concise", "detailed"], index=0)
        with col2:
            send = st.button("Send")

        # When user submits
        if send and user_input:
            with st.spinner("Retrieving context and generating answer..."):
                # retrieve top chunks
                top_ctx = DOC_STORE.similarity_search(doc_choice, user_input, top_k=top_k)
                context = "\n\n---\n\n".join(top_ctx)
                prompt = f"You are an assistant that answers using ONLY the provided context.\nContext:\n{context}\n\nQuestion:\n{user_input}\nInstruction: Answer {mode}ly. Return JSON with an 'answer' field."
                # streaming
                if st.session_state.get("stream_enabled", True):
                    st.write("Streaming response:")
                    placeholder = st.empty()
                    buffer = ""
                    try:
                        for chunk in OLLAMA.stream_generate(prompt, timeout=120):
                            buffer += chunk
                            placeholder.markdown(buffer)
                        final = buffer
                    except Exception as e:
                        st.error(f"Ollama stream error: {e}")
                        final = None
                else:
                    try:
                        final = OLLAMA.generate(prompt, timeout=60)
                    except Exception as e:
                        st.error(f"Ollama error: {e}")
                        final = None

                st.session_state.history.append({"question": user_input, "answer": final})
                st.experimental_rerun()

        # Display history (most recent last)
        st.subheader("History")
        for item in st.session_state.history[::-1]:
            st.markdown(f"**Q:** {item['question']}")
            st.markdown(f"**A:** {item['answer']}")
# rag_utils.py
import os
import hashlib
import json
from pathlib import Path
from typing import List
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
import numpy as np

# Simple on-disk DocumentStore:
class DocumentStore:
    def __init__(self, cache_dir: str = ".emb_cache", chunk_size: int = 1000, chunk_overlap: int = 200):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self._sbert = None

    def _sbert(self):
        if self._sbert is None:
            self._sbert = SentenceTransformer(os.getenv("SBERT_MODEL", "all-MiniLM-L6-v2"), device="cpu")
        return self._sbert

    def _file_hash(self, path: str):
        h = hashlib.sha256()
        with open(path, "rb") as f:
            while True:
                chunk = f.read(8192)
                if not chunk:
                    break
                h.update(chunk)
        return h.hexdigest()

    def add_file(self, path: str):
        path = str(path)
        if not Path(path).exists():
            raise FileNotFoundError(path)
        # build chunks and embeddings if missing
        key = self._file_hash(path)
        meta = self.cache_dir / f"{key}.meta.json"
        emb = self.cache_dir / f"{key}.emb.json"
        if meta.exists() and emb.exists():
            return
        chunks = build_chunks_for_file(path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
        embs = self._generate_embeddings(chunks)
        meta_data = {"path": path, "chunk_count": len(chunks), "chunk_size": self.chunk_size}
        meta.write_text(json.dumps(meta_data, ensure_ascii=False))
        emb.write_text(json.dumps(embs))
        return True

    def rebuild_embeddings(self, path: str):
        key = self._file_hash(path)
        meta = self.cache_dir / f"{key}.meta.json"
        emb = self.cache_dir / f"{key}.emb.json"
        chunks = build_chunks_for_file(path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
        embs = self._generate_embeddings(chunks)
        meta_data = {"path": path, "chunk_count": len(chunks), "chunk_size": self.chunk_size}
        meta.write_text(json.dumps(meta_data, ensure_ascii=False))
        emb.write_text(json.dumps(embs))
        return True

    def list_docs(self) -> List[str]:
        docs = []
        for m in self.cache_dir.glob("*.meta.json"):
            try:
                d = json.loads(m.read_text())
                docs.append(d["path"])
            except Exception:
                continue
        return docs

    def get_meta(self, path: str):
        key = self._file_hash(path)
        meta = self.cache_dir / f"{key}.meta.json"
        if meta.exists():
            return json.loads(meta.read_text())
        return {}

    def get_chunks(self, path: str) -> List[str]:
        key = self._file_hash(path)
        emb = self.cache_dir / f"{key}.emb.json"
        if not emb.exists():
            # trigger generation
            self.add_file(path)
        # read meta to get chunks count, then rebuild chunks
        return build_chunks_for_file(path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)

    def _generate_embeddings(self, chunks: List[str]):
        model = SentenceTransformer(os.getenv("SBERT_MODEL", "all-MiniLM-L6-v2"), device="cpu")
        all_embs = []
        batch = 64
        for i in range(0, len(chunks), batch):
            sub = chunks[i:i+batch]
            embs = model.encode(sub, show_progress_bar=False, convert_to_numpy=True)
            for row in embs:
                all_embs.append(list(map(float, row.tolist())))
        return all_embs

    def similarity_search(self, path: str, query: str, top_k: int = 6):
        # load cached embeddings and chunks
        key = self._file_hash(path)
        emb_file = self.cache_dir / f"{key}.emb.json"
        if not emb_file.exists():
            self.add_file(path)
        embs = json.loads(emb_file.read_text())
        chunks = build_chunks_for_file(path, chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)
        vectors = np.array(embs)
        q = np.array(self._generate_embeddings([query])[0])
        norms = np.linalg.norm(vectors, axis=1) * (np.linalg.norm(q) + 1e-12)
        sims = vectors.dot(q) / norms
        idx = np.argsort(-sims)[:top_k]
        return [chunks[int(i)] for i in idx]

# helper: extract text + images -> pages
def extract_text_from_pdf(path: str):
    pages = []
    try:
        doc = fitz.open(path)
        for p in doc:
            try:
                txt = p.get_text("text") or ""
                pages.append(txt)
            except Exception:
                pages.append("")
    except Exception:
        return []
    return pages

def build_chunks_for_file(path: str, chunk_size: int = 1000, chunk_overlap: int = 200):
    pages = extract_text_from_pdf(path)
    # fallback if no pages (empty): return []
    if not pages:
        return []
    chunks = []
    for p in pages:
        s = (p or "").strip()
        if not s:
            continue
        i = 0
        while i < len(s):
            chunks.append(s[i:i+chunk_size])
            i += chunk_size - chunk_overlap
    return chunks
    # ollama_utils.py
import os
import requests
import time
from typing import Generator, Optional

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://127.0.0.1:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3")

class OllamaClient:
    def __init__(self, host: Optional[str] = None, model: Optional[str] = None):
        self.host = host or OLLAMA_HOST
        self.model = model or OLLAMA_MODEL
        self.session = requests.Session()

    def generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.0, timeout: int = 60) -> str:
        url = f"{self.host}/api/generate"
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_length": int(max_tokens),
            "temperature": float(temperature),
            "stream": False,
        }
        r = self.session.post(url, json=payload, timeout=timeout)
        r.raise_for_status()
        try:
            data = r.json()
            # resilient extraction
            if isinstance(data, dict):
                if "results" in data and isinstance(data["results"], list) and data["results"]:
                    first = data["results"][0]
                    if isinstance(first, dict) and "content" in first:
                        return first["content"]
                if "response" in data:
                    return data["response"]
                # fallback join
                for v in data.values():
                    if isinstance(v, str):
                        return v
            if isinstance(data, str):
                return data
            return str(data)
        except Exception:
            return r.text

    def stream_generate(self, prompt: str, max_tokens: int = 512, temperature: float = 0.0, timeout: int = 120) -> Generator[str, None, None]:
        """
        Stream text from Ollama. Ollama's streaming shape may vary; this function tries to read streaming JSON lines
        or streamed text. It yields incremental chunks (strings).
        """
        url = f"{self.host}/api/generate"
        payload = {
            "model": self.model,
            "prompt": prompt,
            "max_length": int(max_tokens),
            "temperature": float(temperature),
            "stream": True,
        }
        # Use stream=True and iterate over lines
        with self.session.post(url, json=payload, stream=True, timeout=timeout) as r:
            r.raise_for_status()
            # Ollama may stream JSON chunks separated by newlines - attempt to read text lines
            for line in r.iter_lines(decode_unicode=True):
                if not line:
                    continue
                # try to parse JSON-like pieces
                try:
                    # many Ollama versions send JSON objects per line
                    import json
                    obj = json.loads(line)
                    # attempt to extract content fields
                    if isinstance(obj, dict):
                        if "content" in obj:
                            yield obj["content"]
                        elif "response" in obj:
                            yield obj["response"]
                        else:
                            # join string values
                            for v in obj.values():
                                if isinstance(v, str):
                                    yield v
                    else:
                        yield str(obj)
                except Exception:
                    # not JSON — treat as plain chunk
                    yield line
streamlit
sentence-transformers
torch
numpy
pymupdf
requests
